{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iko-ai/ibibio-tts/blob/master/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "519BayIw16w9",
        "outputId": "9d26f302-4c70-48f8-ecd8-25703478223f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import csv\n",
        "import random\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "CV6hrveq2eyI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(os.listdir(\"/content/drive/MyDrive/dataset/wav_split/wavs\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diC0lprL5UcB",
        "outputId": "1431bf71-bf96-4098-81e6-8ad5e74cdfe2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "input_excel_path = '/content/metadata.xlsx'\n",
        "output_csv_path = '/content/drive/MyDrive/dataset/metadata.csv'\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel(input_excel_path, engine='openpyxl')\n",
        "\n",
        "# Save the dataframe as a CSV file\n",
        "df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
        "\n",
        "print(\"Excel file has been successfully converted to CSV.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4Xl15zB4vSr",
        "outputId": "e67949ef-48a7-4ac7-e828-1d5e301d5c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excel file has been successfully converted to CSV.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "input_csv_path = '/content/drive/MyDrive/dataset/metadata.csv'\n",
        "input_wavs_dir = '/content/drive/MyDrive/dataset/wav'\n",
        "output_dir = '/content/drive/MyDrive/dataset/wav_split'\n",
        "train_output_path = os.path.join(output_dir, 'train.txt')\n",
        "val_output_path = os.path.join(output_dir, 'val.txt')"
      ],
      "metadata": {
        "id": "a5kFSaG65nPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PTiaR2j4vs2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the output directory if it doesn't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Read the input CSV\n",
        "with open(input_csv_path, 'r') as csv_file:\n",
        "    reader = csv.DictReader(csv_file)\n",
        "    data = list(reader)\n",
        "\n",
        "# Shuffle the data\n",
        "random.shuffle(data)\n",
        "\n",
        "# Split the data into 80% training and 20% validation\n",
        "split_index = int(0.8 * len(data))\n",
        "train_data = data[:split_index]\n",
        "val_data = data[split_index:]\n",
        "\n",
        "# Function to write data to a text file\n",
        "def write_data(file_path, data):\n",
        "    with open(file_path, 'w') as file:\n",
        "        for row in data:\n",
        "            # Extract just the filename from the full file path\n",
        "            filename = os.path.basename(row['Filepath'])\n",
        "            transcription = row['Transcription']\n",
        "            file.write(f\"wavs/{filename}|{transcription}\\n\")"
      ],
      "metadata": {
        "id": "SrqJKEJs2NA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the training and validation data to their respective files\n",
        "write_data(train_output_path, train_data)\n",
        "write_data(val_output_path, val_data)"
      ],
      "metadata": {
        "id": "AvpNLDAn5rSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/iko-ai/ibibio-tts.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuIFNM9D5tC5",
        "outputId": "77712da3-758b-4345-c0a5-51efe2dce168"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ibibio-tts'...\n",
            "remote: Enumerating objects: 16671, done.\u001b[K\n",
            "remote: Counting objects: 100% (440/440), done.\u001b[K\n",
            "remote: Compressing objects: 100% (229/229), done.\u001b[K\n",
            "remote: Total 16671 (delta 244), reused 370 (delta 210), pack-reused 16231\u001b[K\n",
            "Receiving objects: 100% (16671/16671), 12.17 MiB | 28.85 MiB/s, done.\n",
            "Resolving deltas: 100% (13357/13357), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd ../"
      ],
      "metadata": {
        "id": "Euf_Uq4yJ7C-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd ibibio-texttospeech/codes\n",
        "#%cd codes\n",
        "%cd /content/ibibio-tts/codes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg4QnjEu6Ngz",
        "outputId": "472e7419-c046-4d55-f538-67da911e155a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ibibio-tts/codes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.laxed.txt"
      ],
      "metadata": {
        "id": "QwsDLpEm6Swl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/Gatozu35/tortoise-tts/resolve/main/dvae.pth -O /content/ibibio-tts/experiments/dvae.pth\n",
        "!wget https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/autoregressive.pth -O /content/ibibio-tts/experiments/autoregressive.pth"
      ],
      "metadata": {
        "id": "qixhVDpCPilC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make sure the waveform audio has a sampling rate of 22.05kHz\n",
        "Install librosa using the following command (automatically installs soundfile as well):\n",
        "pip install librosa"
      ],
      "metadata": {
        "id": "ui5jUdh7ST4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(os.listdir(\"/content/drive/MyDrive/dataset/wav_split/wavs\")))"
      ],
      "metadata": {
        "id": "UY8hZERevF1K",
        "outputId": "def0090a-a6c3-40cd-f08d-5a1340c653a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "def resample_wav_file(input_file, target_sampling_rate=22050):\n",
        "    # Load audio file\n",
        "    audio, sampling_rate = librosa.load(input_file, sr=None)\n",
        "\n",
        "    # Check if the sampling rate matches the target\n",
        "    if sampling_rate != target_sampling_rate:\n",
        "\n",
        "        # Resample audio to the target sampling rate\n",
        "        audio_resampled = librosa.resample(audio, orig_sr=sampling_rate, target_sr=target_sampling_rate)\n",
        "\n",
        "        # Overwrite the input file with the resampled audio\n",
        "        sf.write(input_file, audio_resampled, target_sampling_rate)"
      ],
      "metadata": {
        "id": "ja7Stce2QfY6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resample all audio samples to 22.05kHz\n",
        "dataset_path = '/content/drive/MyDrive/dataset/wav_split/wavs'\n",
        "for wav_file in glob(dataset_path + \"*.wav\"):\n",
        "    resample_wav_file(input_file)"
      ],
      "metadata": {
        "id": "m5XZqSloQf9o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tune THE AUTOREGRESSIVE MODEL"
      ],
      "metadata": {
        "id": "CpAZSlxqTRUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/ibibio-tts/codes/train.py -opt /content/ibibio-tts/experiments/custom_language_gpt.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQtEm55VQgLK",
        "outputId": "3490bfb0-8622-4ecb-9505-b5c5cd46faa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Disabled distributed training.\n",
            "Path already exists. Rename it to [/content/ibibio-tts/experiments/custom_language_gpt_archived_240606-225245]\n",
            "24-06-06 22:52:45.655 - INFO:   name: custom_language_gpt\n",
            "  model: extensibletrainer\n",
            "  scale: 1\n",
            "  gpu_ids: [0]\n",
            "  start_step: 0\n",
            "  checkpointing_enabled: True\n",
            "  fp16: False\n",
            "  use_8bit: True\n",
            "  wandb: True\n",
            "  use_tb_logger: False\n",
            "  datasets:[\n",
            "    train:[\n",
            "      name: train_dataset\n",
            "      n_workers: 8\n",
            "      batch_size: 128\n",
            "      mode: paired_voice_audio\n",
            "      path: /content/drive/MyDrive/dataset/wav_split/train.txt\n",
            "      fetcher_mode: ['lj']\n",
            "      phase: train\n",
            "      max_wav_length: 255995\n",
            "      max_text_length: 200\n",
            "      sample_rate: 22050\n",
            "      load_conditioning: True\n",
            "      num_conditioning_candidates: 2\n",
            "      conditioning_length: 44000\n",
            "      use_bpe_tokenizer: True\n",
            "      load_aligned_codes: False\n",
            "      tokenizer_vocab: /content/drive/MyDrive/dataset/custom_language_tokenizer.json\n",
            "      data_type: img\n",
            "    ]\n",
            "    val:[\n",
            "      name: val_dataset\n",
            "      n_workers: 1\n",
            "      batch_size: 64\n",
            "      mode: paired_voice_audio\n",
            "      path: /content/drive/MyDrive/dataset/wav_split/val.txt\n",
            "      fetcher_mode: ['lj']\n",
            "      phase: val\n",
            "      max_wav_length: 255995\n",
            "      max_text_length: 200\n",
            "      sample_rate: 22050\n",
            "      load_conditioning: True\n",
            "      num_conditioning_candidates: 2\n",
            "      conditioning_length: 44000\n",
            "      use_bpe_tokenizer: True\n",
            "      load_aligned_codes: False\n",
            "      tokenizer_vocab: /content/drive/MyDrive/dataset/custom_language_tokenizer.json\n",
            "      data_type: img\n",
            "    ]\n",
            "  ]\n",
            "  steps:[\n",
            "    gpt_train:[\n",
            "      training: gpt\n",
            "      loss_log_buffer: 500\n",
            "      optimizer: adamw\n",
            "      optimizer_params:[\n",
            "        lr: 1e-05\n",
            "        triton: False\n",
            "        weight_decay: 0.01\n",
            "        beta1: 0.9\n",
            "        beta2: 0.96\n",
            "      ]\n",
            "      clip_grad_eps: 4\n",
            "      injectors:[\n",
            "        paired_to_mel:[\n",
            "          type: torch_mel_spectrogram\n",
            "          mel_norm_file: /content/ibibio-tts/experiments/clips_mel_norms.pth\n",
            "          in: wav\n",
            "          out: paired_mel\n",
            "        ]\n",
            "        paired_cond_to_mel:[\n",
            "          type: for_each\n",
            "          subtype: torch_mel_spectrogram\n",
            "          mel_norm_file: /content/ibibio-tts/experiments/clips_mel_norms.pth\n",
            "          in: conditioning\n",
            "          out: paired_conditioning_mel\n",
            "        ]\n",
            "        to_codes:[\n",
            "          type: discrete_token\n",
            "          in: paired_mel\n",
            "          out: paired_mel_codes\n",
            "          dvae_config: /content/ibibio-tts/experiments/train_diffusion_vocoder_22k_level.yml\n",
            "        ]\n",
            "        paired_fwd_text:[\n",
            "          type: generator\n",
            "          generator: gpt\n",
            "          in: ['paired_conditioning_mel', 'padded_text', 'text_lengths', 'paired_mel_codes', 'wav_lengths']\n",
            "          out: ['loss_text_ce', 'loss_mel_ce', 'logits']\n",
            "        ]\n",
            "      ]\n",
            "      losses:[\n",
            "        text_ce:[\n",
            "          type: direct\n",
            "          weight: 0.01\n",
            "          key: loss_text_ce\n",
            "        ]\n",
            "        mel_ce:[\n",
            "          type: direct\n",
            "          weight: 1\n",
            "          key: loss_mel_ce\n",
            "        ]\n",
            "      ]\n",
            "    ]\n",
            "  ]\n",
            "  networks:[\n",
            "    gpt:[\n",
            "      type: generator\n",
            "      which_model_G: unified_voice2\n",
            "      kwargs:[\n",
            "        layers: 30\n",
            "        model_dim: 1024\n",
            "        heads: 16\n",
            "        max_text_tokens: 402\n",
            "        max_mel_tokens: 604\n",
            "        max_conditioning_inputs: 2\n",
            "        mel_length_compression: 1024\n",
            "        number_text_tokens: 256\n",
            "        number_mel_codes: 8194\n",
            "        start_mel_token: 8192\n",
            "        stop_mel_token: 8193\n",
            "        start_text_token: 255\n",
            "        train_solo_embeddings: False\n",
            "        use_mel_codes_as_input: True\n",
            "        checkpointing: True\n",
            "        tortoise_compat: True\n",
            "      ]\n",
            "    ]\n",
            "  ]\n",
            "  path:[\n",
            "    pretrain_model_gpt: /content/ibibio-tts/experiments/autoregressive.pth\n",
            "    strict_load: True\n",
            "    root: /content/ibibio-tts\n",
            "    experiments_root: /content/ibibio-tts/experiments/custom_language_gpt\n",
            "    models: /content/ibibio-tts/experiments/custom_language_gpt/models\n",
            "    training_state: /content/ibibio-tts/experiments/custom_language_gpt/training_state\n",
            "    log: /content/ibibio-tts/experiments/custom_language_gpt\n",
            "    val_images: /content/ibibio-tts/experiments/custom_language_gpt/val_images\n",
            "  ]\n",
            "  train:[\n",
            "    niter: 500\n",
            "    warmup_iter: -1\n",
            "    mega_batch_factor: 4\n",
            "    val_freq: 500\n",
            "    default_lr_scheme: MultiStepLR\n",
            "    gen_lr_steps: [500, 1000, 1400, 1800]\n",
            "    lr_gamma: 0.5\n",
            "    ema_enabled: False\n",
            "  ]\n",
            "  eval:[\n",
            "    pure: True\n",
            "  ]\n",
            "  logger:[\n",
            "    print_freq: 100\n",
            "    save_checkpoint_freq: 500\n",
            "    visuals: ['gen', 'mel']\n",
            "    visual_debug_rate: 500\n",
            "    is_mel_spectrogram: True\n",
            "    disable_state_saving: True\n",
            "  ]\n",
            "  upgrades:[\n",
            "    number_of_checkpoints_to_save: 0\n",
            "    number_of_states_to_save: 0\n",
            "  ]\n",
            "  is_train: True\n",
            "  dist: False\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "0Q2t452-wJqj",
        "outputId": "99a8d512-8a81-4a33-cedc-2662067ee951",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.5.0-py2.py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.5/289.5 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.6.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.5.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.33.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdS6XIiqcdLx",
        "outputId": "8db7dd3e-eea3-4b74-e3a3-5a7b7b662b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.33.2 in /usr/local/lib/python3.10/dist-packages (4.33.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (0.23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.2) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.2) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.2) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEmI8DPrTiNb",
        "outputId": "e821a4ea-f886-48bb-f1b1-6836dd7ba276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ibibio-tts/codes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tokenizers\n",
        "print(tokenizers.__version__)\n"
      ],
      "metadata": {
        "id": "SjPOt6ZGUKXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcd256e7-4849-48fa-9c69-ad034fa4d63a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "\n",
        "\n",
        "def remove_extraneous_punctuation(word):\n",
        "    replacement_punctuation = {\n",
        "        '{': '(', '}': ')',\n",
        "        '[': '(', ']': ')',\n",
        "        '`': '\\'', '—': '-',\n",
        "        '—': '-', '`': '\\'',\n",
        "        'ʼ': '\\''\n",
        "    }\n",
        "    replace = re.compile(\"|\".join([re.escape(k) for k in sorted(replacement_punctuation, key=len, reverse=True)]), flags=re.DOTALL)\n",
        "    word = replace.sub(lambda x: replacement_punctuation[x.group(0)], word)\n",
        "    extraneous = re.compile(r'^[@#%_=\\$\\^&\\*\\+\\\\]$')\n",
        "    word = extraneous.sub('', word)\n",
        "    return word\n",
        "\n",
        "with open('transcriptions.txt', 'r', encoding='utf-8') as at:\n",
        "    ttsd = at.readlines()\n",
        "    allowed_characters_re = re.compile(r'^[a-zịñọʌ!:;\"/, \\-\\(\\)\\.\\'\\?ʼ]+$')\n",
        "\n",
        "    def preprocess_word(word, report=False):\n",
        "        word = remove_extraneous_punctuation(word)\n",
        "        if not bool(allowed_characters_re.match(word)):\n",
        "            if report and word:\n",
        "                print(f\"REPORTING: '{word}'\")\n",
        "            return ''\n",
        "        return word\n",
        "\n",
        "    def batch_iterator(batch_size=1000):\n",
        "        print(\"Processing ASR texts.\")\n",
        "        for i in range(0, len(ttsd), batch_size):\n",
        "            yield [preprocess_word(t, True) for t in ttsd[i:i+batch_size]]\n",
        "\n",
        "    trainer = BpeTrainer(special_tokens=['[STOP]', '[UNK]', '[SPACE]'], vocab_size=255)\n",
        "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "    tokenizer.train_from_iterator(batch_iterator(), trainer, length=len(ttsd))\n",
        "    tokenizer.save('./custom_language_tokenizer.json')\n",
        "\n",
        "    # Save the tokenizer to hub\n",
        "    tokenizer.push_to_hub(repo_id='aidystark/ibibio-tokenizer')"
      ],
      "metadata": {
        "id": "QIXVBNHEav3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YZF5QuOAf5Bp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}